{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o40y8HxDYE3G"
   },
   "source": [
    "# Time Series Anomaly Detection using LSTM Autoencoders with PyTorch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3RY_N3gOmfDi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "#RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to set random seed for randomness splitting data and randomness initial of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5ZI34UQQSD0h"
   },
   "outputs": [],
   "source": [
    "def set_seed( RANDOM_SEED):\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # using GPU for faster computation if Cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7l8ob3P2aVtq",
    "outputId": "1a1147be-7fb6-4ca4-b3bf-ba031d93cbb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from gdown) (4.66.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDlfeY2VAYdU",
    "outputId": "9cebfb20-2a01-4a3f-a67a-72bd6c245e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT\n",
      "To: /home/lamhuutoan.nguyen/smd_project/SMD-Autoencoder/ECG5000.zip\n",
      "100%|██████████████████████████████████████| 10.6M/10.6M [00:00<00:00, 19.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DFWsBcdWjDkU"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # using GPU for faster computation if Cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import scipy.io.arff as arff\n",
    "\n",
    "\n",
    "# Load training data\n",
    "with open('ECG5000_TRAIN.arff', 'r') as f:\n",
    "    train_data, train_meta = arff.loadarff(f)\n",
    "\n",
    "# Load test data\n",
    "with open('ECG5000_TEST.arff', 'r') as f:\n",
    "    test_data, test_meta = arff.loadarff(f)\n",
    "\n",
    "# Convert the ARFF data to Pandas DataFrames\n",
    "train = pd.DataFrame(train_data)\n",
    "test = pd.DataFrame(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 141)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train, test])  ## combine train and test data\n",
    "df = df.sample(frac=1.0) ## shuffle df (frac = 1.0 means 100%)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NORMAL = 1\n",
    "\n",
    "class_names = ['Normal','R on T','PVC','SP','UB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = list(df.columns)\n",
    "new_columns[-1] = 'target'\n",
    "df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 141)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df = df[df.target == str(CLASS_NORMAL)] #.drop(labels='target', axis=1)\n",
    "#normal_target_df = df[df.target == str(CLASS_NORMAL)]['target'].copy()\n",
    "normal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2081, 141)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_df = df[df.target != str(CLASS_NORMAL)] #.drop(labels='target', axis=1)\n",
    "#anomaly_df_target = df[df.target != str(CLASS_NORMAL)]['target'].copy()\n",
    "anomaly_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(RANDOM_SEED, train):\n",
    "    train_df, val_df = train_test_split(\n",
    "    train,\n",
    "    test_size=0.3,\n",
    "    random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    val_df, test_df = train_test_split(\n",
    "    val_df,\n",
    "    test_size = 0.2,\n",
    "    random_state = RANDOM_SEED\n",
    "    )\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_target(df):\n",
    "  df = df.drop(labels=\"target\",axis =1)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, default_collate\n",
    "class CustomeDataSet(Dataset):  # Custom dataset used for train- and val-dataset\n",
    "  \n",
    "    def __init__(self,data):\n",
    "        data = data.astype(np.float32).to_numpy()\n",
    "        self.dataset = [torch.tensor(s).unsqueeze(1).float() for s in data]     \n",
    "    # support indexing such that dataset[i] can  \n",
    "    # be used to get i-th sample \n",
    "    def __getitem__(self, index): \n",
    "        return self.dataset[index] \n",
    "        \n",
    "    # we can call len(dataset) to return the size \n",
    "    def __len__(self): \n",
    "        return len(self.dataset) \n",
    "    def getShape(self):\n",
    "    \n",
    "        n_features = self.dataset[0].shape[1]\n",
    "        seq_len = self.dataset[0].shape[0]\n",
    "        return seq_len, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataSet(Dataset): # use for test-set cause we want to also modify the test-label to match the test_set for calculating the AUC\n",
    "  \n",
    "    def __init__(self,data,target):\n",
    "        data = data.astype(np.float32).to_numpy()\n",
    "        self.dataset = [torch.tensor(s).unsqueeze(1).float() for s in data]\n",
    "        self.target = target\n",
    "    # support indexing such that dataset[i] can  \n",
    "    # be used to get i-th sample \n",
    "    def __getitem__(self, index): \n",
    "        current_data = self.dataset[index]\n",
    "        value = self.target.iloc[index]\n",
    "        label = float(value)\n",
    "        current_target = torch.tensor(label, dtype = torch.float32)\n",
    "        return {\n",
    "            \"x\" : current_data,\n",
    "            \"y\" : current_target\n",
    "        }\n",
    "        \n",
    "    # we can call len(dataset) to return the size \n",
    "    def __len__(self): \n",
    "        return len(self.dataset) \n",
    "    def getShape(self):\n",
    "    \n",
    "        n_features = self.dataset[0].shape[1]\n",
    "        seq_len = self.dataset[0].shape[0]\n",
    "        return seq_len, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_Loader_create(train_df, val_df, train_test_df, anomaly_df):\n",
    "    train_df = drop_target(train_df)\n",
    "    val_df = drop_target(val_df)\n",
    "    test_df = pd.concat( [train_test_df, anomaly_df])\n",
    "    test_target_df = test_df[\"target\"]\n",
    "    test_final_df = drop_target(test_df)\n",
    "    \n",
    "    train_dataset = CustomeDataSet(train_df)\n",
    "    val_dataset = CustomeDataSet(val_df)\n",
    "    test_dataset = TestDataSet(test_final_df,test_target_df)\n",
    "    \n",
    "    train_Loader = DataLoader(train_dataset, shuffle=True, batch_size = 5)\n",
    "    val_Loader = DataLoader(val_dataset, shuffle=True, batch_size = 1)\n",
    "    test_Loader = DataLoader(test_dataset, shuffle=False, batch_size = 1)\n",
    "    #test_Loader = create_test_dataset(test_df)\n",
    "    seq_len , n_features =  train_dataset.getShape()\n",
    "    return train_Loader, seq_len, n_features, val_Loader, test_Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT4BekX2g4L_"
   },
   "source": [
    "Each Time Series will be converted to a 2D Tensor in the shape *sequence length* x *number of features* (140x1 in our case).\n",
    "\n",
    "Let's create some datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "X_f1WaTJhiXy"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, n_features, rnn_type, number_layers, embedding_dim=32, n_stages =2):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "    self.rnn_type = rnn_type\n",
    "    self.n_stages = n_stages\n",
    "    self.number_layers = number_layers\n",
    "    rnn_dict = {\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "\n",
    "    self.rnn_stages = nn.ModuleList() # a list to hold RNN_stages but not the last stage\n",
    "    if self.n_stages ==1:\n",
    "        self.output_stage = rnn_dict[self.rnn_type.upper()]( # the last stage\n",
    "          input_size=self.n_features,\n",
    "          hidden_size= self.embedding_dim,\n",
    "          num_layers= self.number_layers,\n",
    "          batch_first=True\n",
    "        )\n",
    "    else:\n",
    "        # input_size > hidden_size, cause in Encoder we want to compress the data, reduce the dimension into a  \n",
    "        # representation, which can capture the most significant features and will be passed to the decoder\n",
    "        self.rnn_stages.append(  # append the first stage cause input_size = n_features\n",
    "          rnn_dict[self.rnn_type.upper()](\n",
    "            input_size=self.n_features,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers= self.number_layers,\n",
    "            batch_first=True\n",
    "          )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        for i in range(1,self.n_stages-1):# append the rest stages except the last one\n",
    "            self.rnn_stages.append(\n",
    "            rnn_dict[self.rnn_type.upper()](\n",
    "                input_size=self.hidden_dim, # diff from the first stage\n",
    "                hidden_size=self.hidden_dim,\n",
    "                num_layers= self.number_layers,\n",
    "                batch_first=True\n",
    "              )\n",
    "            )\n",
    "        \n",
    "        self.output_stage = rnn_dict[self.rnn_type.upper()]( # the last stage\n",
    "          input_size=self.hidden_dim,\n",
    "          hidden_size= self.embedding_dim,\n",
    "          num_layers= self.number_layers,\n",
    "          batch_first=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    if x.size(0) ==1: #(checking if the batch_num equal 1)\n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "    else :\n",
    "        x = x.reshape((x.size(0), self.seq_len, self.n_features)) #(cause batch is set by DataLoader)\n",
    "    if self.rnn_type.upper() == \"GRU\" :\n",
    "      for stage in self.rnn_stages:\n",
    "        x,_ = stage(x)\n",
    "      x,hidden_n = self.output_stage(x)\n",
    "\n",
    "      # x, _,  = self.rnn1(x) # x, hidden_n\n",
    "      # x, hidden_n  = self.rnn2(x) # x, hidden_n\n",
    "    else:\n",
    "      for stage in self.rnn_stages:\n",
    "        x,(_,_) = stage(x)\n",
    "      x,(hidden_n,_) = self.output_stage(x)\n",
    "\n",
    "      # x, (_,_)  = self.rnn1(x) # x, hidden_n\n",
    "      # x, (hidden_n,_)  = self.rnn2(x) # x, hidden_n\n",
    "    hidden_n = hidden_n[-1]\n",
    "    if x.size(0) ==1: #(checking if the batch_num equal 1)\n",
    "        return hidden_n.reshape((self.n_features, self.embedding_dim))\n",
    "    else:\n",
    "        return hidden_n.reshape((x.size(0), self.embedding_dim)) #( dimension of ( batch, embbeding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DysklqYmxTib"
   },
   "source": [
    "The *Encoder* uses two LSTM layers to compress the Time Series data input.\n",
    "\n",
    "Next, we'll decode the compressed representation using a *Decoder*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AdEft7l3hk6S"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, rnn_type, number_layers, input_dim=32, n_features=1, n_stages =2 ):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "    self.hidden_dim, self.n_features = 2*input_dim, n_features\n",
    "    self.rnn_type = rnn_type\n",
    "    self.n_stages = n_stages\n",
    "    self.number_layers = number_layers  \n",
    "\n",
    "\n",
    "    rnn_dict = {\n",
    "            'LSTM': nn.LSTM,\n",
    "            'GRU': nn.GRU\n",
    "        }\n",
    "    # input_size < hidden_size cause we want to reconstruct the original input from the passed representation\n",
    "    # of the encoder\n",
    "    self.rnn_stages = nn.ModuleList()\n",
    "    for _ in range(self.n_stages -1 ):\n",
    "        self.rnn_stages.append(\n",
    "          rnn_dict[self.rnn_type.upper()]( #GRU\n",
    "              input_size=input_dim,\n",
    "              hidden_size=input_dim,\n",
    "              num_layers=self.number_layers,\n",
    "              batch_first=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "    self.rnn_stages.append(rnn_dict[self.rnn_type.upper()]( #GRU\n",
    "      input_size=input_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=self.number_layers,\n",
    "      batch_first=True\n",
    "    ))\n",
    "\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, self.n_features) # last stage\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size = x.size(0)\n",
    "    if x.size(0) ==1: #(checking if the batch_num equal 1)\n",
    "        x = x.repeat(self.seq_len, self.n_features)\n",
    "        x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "    else:\n",
    "        x = x.repeat(1,self.seq_len, 1)\n",
    "        x = x.reshape((batch_size, self.seq_len, self.input_dim))\n",
    "    if self.rnn_type.upper() == \"GRU\" :\n",
    "        for stage in self.rnn_stages:\n",
    "            x, _ = stage(x)\n",
    "        # x, _ = self.rnn1(x) # x, hidden_n\n",
    "        # x, _ = self.rnn2(x) # x, hidden_n\n",
    "    else:\n",
    "        for stage in self.rnn_stages:\n",
    "            x, (_,_) = stage(x)\n",
    "        #x, (_, _) = self.rnn1(x)\n",
    "        # x, (_, _) = self.rnn2(x)\n",
    "    if x.size(0) ==1: #(checking if the batch_num equal 1)\n",
    "        x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "    else:\n",
    "        x = x.reshape((batch_size,self.seq_len, self.hidden_dim))\n",
    "\n",
    "    return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUMb2NGZyTmi"
   },
   "source": [
    "Our Decoder contains two LSTM layers and an output layer that gives the final reconstruction.\n",
    "\n",
    "Time to wrap everything into an easy to use module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vgUChGd_A-Bv"
   },
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, n_features, number_layers,type, n_stages, embedding_dim=64):\n",
    "    super(RecurrentAutoencoder, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(seq_len, n_features,type,number_layers, embedding_dim, n_stages).to(device)\n",
    "    self.decoder = Decoder(seq_len, type, number_layers, embedding_dim, n_features, n_stages).to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT6Cwq78sOrI"
   },
   "source": [
    "Our Autoencoder passes the input through the Encoder and Decoder. Let's create an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Mo0rvFqRBgnu"
   },
   "outputs": [],
   "source": [
    "def create_model(type,seq_len, n_features, number_layers, n_stages): # we can create a LSTM-autoencoder or GRU-autoencoder\n",
    "  model = RecurrentAutoencoder(seq_len, n_features, number_layers, type, n_stages, 128)\n",
    "  model = model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1ENnubQdnJN"
   },
   "source": [
    "## Training\n",
    "\n",
    "Let's write a helper function for our training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ryEmRvl9DfEj"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_Loader, val_Loader, n_epochs):\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "  criterion = nn.L1Loss(reduction='sum').to(device)  \n",
    "  #history = dict(train=[], val=[])\n",
    "  #patience = 5\n",
    "\n",
    "\n",
    "\n",
    "  best_model_wts = copy.deepcopy(model.state_dict())\n",
    "  best_loss = 10000.0\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    model = model.train()\n",
    "    #num_train = 0\n",
    "    #num_val = 0\n",
    "    train_losses = []\n",
    "    for batch_true in train_Loader:\n",
    "      #num_train += 1\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      batch_true = batch_true.to(device)\n",
    "      batch_pred = model(batch_true)\n",
    "\n",
    "      loss = criterion(batch_pred, batch_true)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_losses.append(loss.item())\n",
    "\n",
    "    val_losses = []\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batch_true in val_Loader:\n",
    "        #num_val +=1\n",
    "\n",
    "        batch_true = batch_true.to(device)\n",
    "        batch_pred = model(batch_true) # feed the model with the input (batch_true) to get the prediction(batch_predict)\n",
    "\n",
    "        loss = criterion(batch_pred, batch_true) # calculate the diff between the input(batch_true)\n",
    "                                            # and the predict of the model ( batch_pred)\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "\n",
    "    #history['train'].append(train_loss)\n",
    "    #history['val'].append(val_loss)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "      best_loss = val_loss\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      #patience =5\n",
    "    #else:\n",
    "      #patience -= 1\n",
    "      #if patience == 0:\n",
    "          #print(f\"break at epoch {epoch}\")\n",
    "          #break\n",
    "    #print(f\"num_train : {num_train}\")\n",
    "    #print(f\"num_val : {num_val}\")\n",
    "\n",
    "    \n",
    "\n",
    "  model.load_state_dict(best_model_wts)\n",
    "  return model.eval() #, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iWQrzV1ASpW"
   },
   "source": [
    "At each epoch, the training process feeds our model with all training examples and evaluates the performance on the validation set. Note that we're using a batch size of 1 (our model sees only 1 sequence at a time). We also record the training and validation set losses during the process.\n",
    "\n",
    "Note that we're minimizing the [L1Loss](https://pytorch.org/docs/stable/nn.html#l1loss), which measures the MAE (mean absolute error). Why? The reconstructions seem to be better than with MSE (mean squared error).\n",
    "\n",
    "We'll get the version of the model with the smallest validation error. Let's do some training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "saamYyUsHdw0"
   },
   "outputs": [],
   "source": [
    "def trained_model(model, train_Loader, val_Loader, n_epochs):\n",
    "  model = train_model(\n",
    "    model,\n",
    "    train_Loader,\n",
    "    val_Loader,\n",
    "    n_epochs\n",
    "  )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmoaNSERn09J"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "Let's store the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tLC_ClIpnv9H"
   },
   "outputs": [],
   "source": [
    "#MODEL_PATH = 'model.pth'\n",
    "\n",
    "#torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6q7H7-ea-Lb"
   },
   "source": [
    "Uncomment the next lines, if you want to download and load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "e4Hxo-Xftiej"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1jEYx5wGsb7Ix8cZAw3l5p5pOwHs3_I9A\n",
    "# model = torch.load('model.pth')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwLujPFXT054"
   },
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "With our model at hand, we can have a look at the reconstruction error on the training set. Let's start by writing a helper function to get predictions from our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_Loader):\n",
    "    predictions, losses, targets = [], [], []\n",
    "    criterion = nn.L1Loss(reduction='sum').to(device)  # Use 'none' to get per-sample loss\n",
    " \n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for data in data_Loader:\n",
    "            batch_true = data[\"x\"]\n",
    "            target = data[\"y\"]\n",
    "            \n",
    "            batch_true = batch_true.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            batch_pred = model(batch_true)\n",
    "            loss = criterion(batch_pred, batch_true)  # Loss per batch\n",
    "            \n",
    "            predictions.append(batch_pred.cpu().numpy().flatten())\n",
    "            losses.append(loss.item())  # Store one loss value per batch\n",
    "            targets.append(target.cpu().numpy().flatten())  # Aggregate targets per batch  \n",
    "\n",
    "    #print(f\"predict {predictions.size}\")\n",
    "    #print(f\"loss { losses[0:10]}\")\n",
    "    #print(f\"number of loop { num}\")\n",
    "\n",
    "    return predictions, losses, targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVnNtIreDXf5"
   },
   "source": [
    "Our function goes through each example in the dataset and records the predictions and losses. Let's get the losses and have a look at them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the AUC_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "z34VKUIosMyF"
   },
   "outputs": [],
   "source": [
    "def cal_AUC (target_data, pred_losses):\n",
    "  target_data = np.array(target_data, dtype=int)\n",
    "  target_data = target_data.flatten()\n",
    "  arg = np.argsort(pred_losses) # arg[i] show the i+1 smallest lossvalue is pred_losses[arg[i]]\n",
    "  rank = np.argsort(arg) # show that rank[i] is the rank of pred_losses[i]\n",
    "  indices = np.arange(len(target_data))[target_data != 1]\n",
    "  #print(\"target_data shape:\", np.shape(target_data))\n",
    "  n1 = len(indices)\n",
    "  n2 = len(target_data) - n1\n",
    "  #print( f\" n1: {n1}\")\n",
    "  #print( f\" n2: {n2}\")\n",
    "  #print(f\" rank: {rank}\")\n",
    "  #print(f\" arg: {arg}\")\n",
    "  #print(\"target shape\")\n",
    "  #print(len(target_data))\n",
    "  #print(len(target_data[0]))  \n",
    "  #print(\"losses shape\")\n",
    "  #print(len(pred_losses))\n",
    "  #print(pred_losses[0])  \n",
    "  \n",
    "  \n",
    "  if n1 ==0 or n2 ==0:\n",
    "    print(\"fail\")\n",
    "    raise Exception(\"Unable to calculate AUC score. Only elements of one class present.\")\n",
    "    #return 0\n",
    "\n",
    "  return (np.mean(rank[indices]) - (n1 +1)/2)/ n2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will write some scripts to run our defined funtions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset with different seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Hf6T1ghB0NqT"
   },
   "outputs": [],
   "source": [
    "def create_data(seed):\n",
    "\n",
    "    RANDOM_SEED = set_seed(seed)\n",
    "    train_df, val_df, train_test_df = train_val_split(RANDOM_SEED, normal_df) \n",
    "    train_Loader, seq_len, n_features, val_Loader, test_Loader  = train_val_test_Loader_create(train_df, val_df, train_test_df, anomaly_df)\n",
    "  \n",
    "    return train_Loader, seq_len, n_features, val_Loader, test_Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model after training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(type,seq_len,n_features, train_Loader, val_Loader,n_epochs,number_layers, n_stages):\n",
    "    model = create_model(type,seq_len, n_features,number_layers, n_stages) # create model \n",
    "    model = trained_model(model, train_Loader, val_Loader,n_epochs) # train model\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caculate the AUC for each model and dataset, also save the Reconstruction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0E5bKpOF13IT"
   },
   "outputs": [],
   "source": [
    "def AUC_and_predLoss(type,seq_len,n_features, train_Loader, val_Loader, test_Loader, n_epochs, number_layers, n_stages):\n",
    "  model = get_model(type,seq_len,n_features, train_Loader, val_Loader, n_epochs,number_layers, n_stages)\n",
    "  _, pred_losses, targets = predict(model, test_Loader)\n",
    "  #Reconstruction_Error[\"GRU\"] = pred_losses\n",
    "  AUC_score = cal_AUC(targets, pred_losses)\n",
    "  #AUC[\"GRU\"].append(AUC_GRU_score)\n",
    "  return pred_losses, AUC_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create n_models ( n GRU-models and n LSTM-models), and train each with n_epochs and each will have n_stages in Encoder and Decoder. So we will have n models of each type with different initial parameter base on Seed and the dataset, which we use to train, validation and test our models, is also split in different ways based on Seed. Thanks to that, we get the average performance results of each type of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "DZ9M5V7LnmI3"
   },
   "outputs": [],
   "source": [
    "def AUC_df(n_epochs, n_models, number_layers, n_stages,file_name, model_type):\n",
    "  AUC = []\n",
    "  Reconstruction_Error= []\n",
    "  for i in range(n_models): # create n_models of GRU, LTSM and calculate AUC to evaluate the perfomences of these 2\n",
    "    train_Loader, seq_len, n_features, val_Loader, test_Loader = create_data(i*5)\n",
    "    ## FIRST the GRU\n",
    "    pred_losses, AUC_score = AUC_and_predLoss(model_type,seq_len,n_features, train_Loader, val_Loader, test_Loader, n_epochs, number_layers, n_stages)\n",
    "    Reconstruction_Error = pred_losses\n",
    "    AUC.append(AUC_score)\n",
    "\n",
    "    AUC_df = pd.DataFrame(AUC)\n",
    "    Reconstruction_Error_df = pd.DataFrame(Reconstruction_Error)\n",
    "    with open(file_name, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([i, AUC_score]) \n",
    "  return AUC_df,Reconstruction_Error_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot a boxplot to show the AUC of LSTM and GRU models, so we can compare the performance of these 2 in general but not for a specific initial parameter or specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "sf_93cp94E-c"
   },
   "outputs": [],
   "source": [
    "def plot_boxplot (auc):\n",
    "\n",
    "  auc_melted = auc.melt(var_name='Model', value_name='AUC')\n",
    "  sns.boxplot(x='Model', y='AUC', data=auc_melted)\n",
    "  plt.title('AUC Scores Comparison Across Models')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_auc(path):\n",
    "    auc = pd.read_csv(path, header = None)\n",
    "    auc = auc.iloc[:,1:3]\n",
    "    auc.columns =  [\"GRU\", \"LSTM\"]\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def duration_auc_reError(n_epochs, n_models, number_layers, n_stages, result_file_name, runtime_file_name,model_type):\n",
    "    start_time = datetime.now()\n",
    "    if model_type == \"GRU\":\n",
    "        auc, Reconstruction_Error = AUC_df(n_epochs, n_models, number_layers, n_stages,result_file_name, model_type)\n",
    "    else:\n",
    "        auc, Reconstruction_Error = AUC_df(n_epochs, n_models, number_layers, n_stages,result_file_name, model_type)\n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    with open(runtime_file_name, 'a', newline='') as file:\n",
    "            file.write(f\"{str(duration)}\\n\")\n",
    "    return duration, auc, Reconstruction_Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TauJUiJfaOo"
   },
   "source": [
    "This function will return the runtime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_1, auc_1, Reconstruction_Error_1 = duration_auc_reError(50, 10, 1, 1,\"GRU_1.csv\",\"GRU_Runtime_1.csv\",\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_1, auc_1, Reconstruction_Error_1 = duration_auc_reError(50, 10, 1, 1,\"LSTM_1.csv\",\"LSTM_runtime_1.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duration_2_GRU, auc_2_GRU, Reconstruction_Error_2_GRU = duration_auc_reError(50, 10, 1, 2,\"GRU_2.csv\",\"GRU_runtime_2.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chay 3 layer toi day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_2_LSTM, auc_2_LSTM, Reconstruction_Error_2_LSTM = duration_auc_reError(50, 10, 1, 2,\"LSTM_2.csv\",\"LSTM_runtime_2.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_3_GRU, auc_3_GRU, Reconstruction_Error_3_GRU = duration_auc_reError(50, 10, 1, 3,\"GRU_3.csv\",\"GRU_runtime_3.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_3_LSTM, auc_3_LSTM, Reconstruction_Error_3_LSTM = duration_auc_reError(50, 10, 1, 3,\"LSTM_3.csv\",\"LSTM_runtime_3.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_4_GRU, auc_4_GRU, Reconstruction_Error_4_GRU = duration_auc_reError(50, 10, 1, 4,\"GRU_4.csv\",\"GRU_runtime_4.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_4_LSTM, auc_4_LSTM, Reconstruction_Error_4_LSTM = duration_auc_reError(50, 10, 1, 4,\"LSTM_4.csv\",\"LSTM_runtime_4.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_5_GRU, auc_5_GRU, Reconstruction_Error_5_GRU = duration_auc_reError(50, 10, 1, 5,\"GRU_5.csv\",\"GRU_runtime_5.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_5_LSTM, auc_5_LSTM, Reconstruction_Error_5_LSTM = duration_auc_reError(50, 10, 1, 5,\"LSTM_5.csv\",\"LSTM_runtime_5.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_6_GRU, auc_6_GRU, Reconstruction_Error_6_GRU = duration_auc_reError(50, 10, 1, 6,\"GRU_6_layer.csv\",\"GRU_runtime_6.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_6_LSTM, auc_6_LSTM, Reconstruction_Error_6_LSTM = duration_auc_reError(50, 10, 1, 6,\"LSTM_6.csv\",\"LSTM_runtime_6.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_7_GRU, auc_7_GRU, Reconstruction_Error_7_GRU = duration_auc_reError(50, 10, 1, 7,\"GRU_7.csv\",\"GRU_runtime_7.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_7_LSTM, auc_7_LSTM, Reconstruction_Error_7_LSTM = duration_auc_reError(50, 10, 1, 7,\"LSTM7.csv\",\"LSTM_runtime_7.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_8_GRU, auc_8_GRU, Reconstruction_Error_8_GRU = duration_auc_reError(50, 10, 1, 8,\"GRU_8.csv\",\"GRU_runtime_8.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_8_LSTM, auc_8_LSTM, Reconstruction_Error_8_LSTM = duration_auc_reError(50, 10, 1, 8,\"LSTM_8.csv\",\"LSTM_runtime_8.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_9_GRU, auc_9_GRU, Reconstruction_Error_9_GRU = duration_auc_reError(50, 10, 1, 9,\"GRU_9.csv\",\"GRU_runtime_9.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_9_LSTM, auc_9_LSTM, Reconstruction_Error_9_LSTM = duration_auc_reError(50, 10, 1, 9,\"LSTM_9.csv\",\"LSTM_runtime_9.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_10_GRU, auc_10_GRU, Reconstruction_Error_10_GRU = duration_auc_reError(50, 10, 1, 10,\"GRU_10.csv\",\"GRU_runtime_10.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "/home/lamhuutoan.nguyen/anaconda3/envs/myenv39/lib/python3.9/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([1, 140, 1])) that is different to the input size (torch.Size([140, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "duration_10_LSTM, auc_10_LSTM, Reconstruction_Error_10_LSTM = duration_auc_reError(50, 10, 1, 10,\"LSTM_10.csv\",\"LSTM_runtime_10.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chay cho 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_10_GRU, auc_10_GRU, Reconstruction_Error_10_GRU = duration_auc_reError(50, 10, 2, 10,\"GRU_test_ten_Stages_two_Layer.csv\",\"GRU_runtime_10_2.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_10_LSTM, auc_10_LSTM, Reconstruction_Error_10_LSTM = duration_auc_reError(50, 10, 2, 10,\"LSTM_test_ten_Stages_two_Layer.csv\",\"LSTM_runtime_10_2.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAY CHO 2 LAYERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_9_LSTM, auc_9_LSTM, Reconstruction_Error_9_LSTM = duration_auc_reError(50, 10, 2, 9,\"LSTM_test_nine_Stages_two_Layer.csv\",\"LSTM_runtime_9_2.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_10_GRU, auc_10_GRU, Reconstruction_Error_10_GRU = duration_auc_reError(50, 10, 2, 10,\"GRU_test_ten_Stages_two_Layer.csv\",\"GRU_runtime_10_2.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_10_LSTM, auc_10_LSTM, Reconstruction_Error_10_LSTM = duration_auc_reError(50, 10, 2, 10,\"LSTM_test_ten_Stages_two_Layer.csv\",\"LSTM_runtime_10_2.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_11_GRU, auc_11_GRU, Reconstruction_Error_11_GRU = duration_auc_reError(50, 10, 11,\"GRU_test_eleven_batch.csv\",\"GRU_runtime_11.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_11_LSTM, auc_11_LSTM, Reconstruction_Error_11_LSTM = duration_auc_reError(50, 10, 11,\"LSTM_test_eleven_batch.csv\",\"LSTM_runtime_11.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_12_GRU, auc_12_GRU, Reconstruction_Error_12_GRU = duration_auc_reError(50, 10, 12,\"GRU_test_twelve_batch.csv\",\"GRU_runtime_12.csv\",\"GRU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_12_LSTM, auc_12_LSTM, Reconstruction_Error_12_LSTM = duration_auc_reError(50, 10, 12,\"LSTM_test_twelve_batch.csv\",\"LSTM_runtime_12.csv\",\"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_boxplot(auc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_2, auc_2, Reconstruction_Error_2 = duration_auc_reError(50, 10, 2,\"test_two_batch.csv\",\"runtime_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(auc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_3, auc_3, Reconstruction_Error_3 = duration_auc_reError(50, 10, 3,\"test_three_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_3 = load_auc(\"test_result_Batch_50/test_three_batch.csv\")\n",
    "auc_melted = auc_3.melt(var_name='Model', value_name='AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_3 = load_auc(\"test_three_batch.csv\")\n",
    "plot_boxplot(auc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_4, auc_4, Reconstruction_Error_4 = duration_auc_reError(50, 10, 4,\"test_four_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_4 = load_auc(\"test_four_batch.csv\")\n",
    "plot_boxplot(auc_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_5, auc_5, Reconstruction_Error_5 = duration_auc_reError(50, 10, 5,\"test_five_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_5 = load_auc(\"test_five_batch.csv\")\n",
    "plot_boxplot(auc_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_6, auc_6, Reconstruction_Error_6 = duration_auc_reError(50, 10, 6,\"test_six_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(auc_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_7, auc_7, Reconstruction_Error_7 = duration_auc_reError(50, 10, 7,\"test_seven_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_7 = load_auc(\"test_seven_batch.csv\")\n",
    "plot_boxplot(auc_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_8, auc_8, Reconstruction_Error_8 = duration_auc_reError(50, 10, 8,\"test_ten_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(auc_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_9, auc_9, Reconstruction_Error_9 = duration_auc_reError(50, 10, 9,\"test_nine_batch.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(auc_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_10, auc_10, Reconstruction_Error_10 = duration_auc_reError(50, 10, 10,\"test_ten_batch.csv\",\"runtime_10.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(auc_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"runtime.csv\", 'a', newline='') as file:\n",
    "            file.write(f\"{str(duration)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw2dm631T4a5"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Using the threshold, we can turn the problem into a simple binary classification task:\n",
    "\n",
    "- If the reconstruction loss for an example is below the threshold, we'll classify it as a *normal* heartbeat\n",
    "- Alternatively, if the loss is higher than the threshold, we'll classify it as an anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the Reconstruction Errors, we can fit the Normal Distribution in it and than find the threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcAobO2KUdcK"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import stats\n",
    "def threshold_set(Reconstruction_Error_df):\n",
    "    threshold = {\"GRU\" : [],\n",
    "                 \"LSTM\" :[]}\n",
    "    mu_GRU, sigma_GRU = stats.norm.fit(Reconstruction_Error_df[\"GRU\"])\n",
    "    threshold[\"GRU\"] = mu_GRU + 3 * sigma_GRU\n",
    "    mu_LSTM, sigma_LSTM = stats.norm.fit(Reconstruction_Error_df[\"LSTM\"])\n",
    "    threshold[\"LSTM\"] = mu_LSTM + 3 * sigma_LSTM\n",
    "\n",
    "    return threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having the threshold, we will create the final model that we wana use for the new datas to detect if that is anomaly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(type, n_epochs, n_stages):\n",
    "    S_N_dataset, seq_len, n_features, V_N_dataset, V1_dataset, T_dataset, V1_target_df, T_target_df = create_data(10000)  \n",
    "    model = create_model(type,seq_len, n_features, n_stages)\n",
    "    final_model = trained_model(model, S_N_dataset, V_N_dataset,n_epochs)    \n",
    "    \n",
    "    return model, T_dataset, T_target_df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
